Data-related artefacts - databricks, parquet and other

# Getting started
## Get deep into Parquet format :
[Dremel: Interactive Analysis of Web-Scale Datasets/Google Research](https://research.google/pubs/pub36632/)

[Dremel made simple with Parquet/Twitter Engineering](https://blog.twitter.com/engineering/en_us/a/2013/dremel-made-simple-with-parquet.html)

[If you have your own Columnar format,stop now and use Parquet](http://www.hpts.ws/papers/2015/lightning/Parquet-HPTS-Lightning-talk.pdf)

[Wrapping one’s head around Repetition and Definition Levels in Dremel, powering BigQuery](https://medium.com/@pallerlaakshay/wrapping-head-around-repetition-and-definition-levels-in-dremel-powering-bigquery-c1a33c9695da)

[Handling Large Amounts of Data with Parquet – Part 1](https://miuv.blog/2018/08/21/handling-large-amounts-of-data-with-parquet-part-1/)

[Parquet docx on Apache](https://parquet.apache.org/documentation/latest/)

[Structure Of Parquet File Format](https://luminousmen.com/post/big-data-file-formats)


## Install conda:
[How to write to a Parquet file in Python](https://easydata.engineering/how-to-write-parquet-file-in-python)


#Azure Data Lake 

[Tutorial: Azure Data Lake Storage Gen2, Azure Databricks & Spark](https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-use-databricks-spark)
